{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet-bottleneck_20190119.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "f5GYrcffhOVd",
        "colab_type": "code",
        "outputId": "4a38a2f8-56d4-481f-c4b0-5b22d485bfb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Activation, merge, Dense, Flatten, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.layers import BatchNormalization, add, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.misc import toimage\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 16s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v069EeDOhdBn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.misc import toimage\n",
        "from sklearn.model_selection import train_test_split\n",
        "cifar10_labels = np.array([\n",
        "    'airplane',\n",
        "    'automobile',\n",
        "    'bird',\n",
        "    'cat',\n",
        "    'deer',\n",
        "    'dog',\n",
        "    'frog',\n",
        "    'horse',\n",
        "    'ship',\n",
        "    'truck'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RlEtq-5nhfbd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# floatに変換しておく\n",
        "# 正規化するために\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# 正規化\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "# RGB各要素について0-255の値で色を表現する、例えば白は（R 255、G 255、B 255）で表現\n",
        "\n",
        "# one-hot vector に変換したほうが都合が良い\n",
        "# たとえば 4 ではなくて [0, 0, 0, 0, 1, 0, 0, 0, 0, 0] のようなデータにする\n",
        "# こうしないとクラス分類ではなくて回帰として扱われてしまいうまくいかない\n",
        "n_classes = 10\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6kjktHFLhghN",
        "colab_type": "code",
        "outputId": "2f6d46f0-8b54-45fa-eb7e-fed3d598490d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1953
        }
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# ResNet のモデルを定義するセル\n",
        "#\n",
        "\n",
        "# inputs と residual 2つの処理結果を受け取って、両者を足す\n",
        "\n",
        "# inputs：ショートカットせずに畳み込みやReLuなどをしたあとの信号\n",
        "# residual：畳み込みなどを通らずショートカットして流れてくる信号のこと、つまりモジュールに入ってくる信号と同じ\n",
        "def _shortcut(inputs, residual):\n",
        "  \n",
        "  # residual のほうのフィルタ数を取得する\n",
        "  # ちなみにデフォルトでは\n",
        "  # _keras_shape[1] 画像の幅\n",
        "  # _keras_shape[2] 画像の高さ\n",
        "  # _keras_shape[3] チャンネル数\n",
        "  # チャンネル数、幅、高さの順番のこともあるが、今回はデフォルトでOK\n",
        "  n_filters = residual._keras_shape[3]\n",
        "  \n",
        "  # inputs と residual とでチャネル数が違うかもしれない。\n",
        "  # そのままだと足せないので、1x1 conv を使って residual 側のフィルタ数に合わせている\n",
        "  shortcut = Convolution2D(n_filters, (1,1), strides=(1,1),kernel_initializer='glorot_normal', padding='same')(inputs)\n",
        " \n",
        "  # 2つを足す\n",
        "  return add([shortcut, residual])\n",
        "\n",
        "\n",
        "# ResBlock を定義\n",
        "# ここでの処理は BatchNorm → ReLU → Conv とシンプルなものにしてあるが、\n",
        "# ここを色々変更する改良案が無数にある\n",
        "###ResBlock とは、ショートカット構造のあるレイヤーの組み合わせ（ブロック）のこと\n",
        "def _resblock(n_filters, strides=(1,1)):\n",
        "  def f(inputs):  \n",
        "    x = Convolution2D(n_filters, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters, (3,3), strides=strides,kernel_initializer='he_normal', padding='same')(x)    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters * 4, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    # 入力そのものと、BN→ReLU→Conv したものとを足す\n",
        "    # この部分がResNetのもっとも重要な点\n",
        "    return _shortcut(inputs, x)\n",
        "  \n",
        "  return f\n",
        "\n",
        "def resnet():\n",
        "    inputs = Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = BatchNormalization()(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=64)(x)\n",
        "#    x = Dropout(0.4)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=64)(x)\n",
        "    \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=128)(x)\n",
        " #   x = Dropout(0.4)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        " #   x = _resblock(n_filters=128)(x)\n",
        "    \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=256)(x)\n",
        "#    x = Dropout(0.4)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "  \n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(10, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "  \n",
        "model = resnet()\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 32, 32, 3)    12          input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 32, 32, 3)    0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 32, 32, 64)   256         activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 32, 32, 64)   256         conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 32, 32, 64)   0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 32, 32, 64)   36928       activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 32, 32, 64)   256         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 32, 32, 64)   0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 32, 32, 256)  16640       activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 32, 32, 256)  1024        conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 32, 32, 256)  1024        activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 32, 32, 256)  0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 32, 32, 256)  0           conv2d_136[0][0]                 \n",
            "                                                                 activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 32, 32, 256)  1024        add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 32, 32, 256)  0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_34 (MaxPooling2D) (None, 16, 16, 256)  0           activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 16, 16, 256)  1024        max_pooling2d_34[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 16, 16, 256)  0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 16, 16, 128)  32896       activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 16, 16, 128)  512         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 16, 16, 128)  0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 16, 16, 128)  147584      activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 16, 16, 128)  512         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 16, 16, 128)  0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 16, 16, 512)  66048       activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 16, 16, 512)  2048        conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 16, 16, 512)  131584      activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 16, 16, 512)  0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 16, 16, 512)  0           conv2d_140[0][0]                 \n",
            "                                                                 activation_144[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 16, 16, 512)  2048        add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 16, 16, 512)  0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling2D) (None, 8, 8, 512)    0           activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 8, 8, 512)    2048        max_pooling2d_35[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 8, 8, 512)    0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 8, 8, 256)    131328      activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 8, 8, 256)    1024        conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 8, 8, 256)    0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 8, 8, 256)    590080      activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 8, 8, 256)    1024        conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 8, 8, 256)    0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 8, 8, 1024)   263168      activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 8, 8, 1024)   4096        conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 8, 8, 1024)   525312      activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 8, 8, 1024)   0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 8, 8, 1024)   0           conv2d_144[0][0]                 \n",
            "                                                                 activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 8, 8, 1024)   4096        add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 8, 8, 1024)   0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_10 (Gl (None, 1024)         0           activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 10)           10250       global_average_pooling2d_10[0][0]\n",
            "==================================================================================================\n",
            "Total params: 1,974,102\n",
            "Trainable params: 1,963,600\n",
            "Non-trainable params: 10,502\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dQNAH21HhmhW",
        "colab_type": "code",
        "outputId": "ba49a9b8-fe16-4c18-9c75-d9b970ca8a6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2602
        }
      },
      "cell_type": "code",
      "source": [
        "#checkpoint\n",
        "callback_op = ModelCheckpoint(filepath='weights.{epoch:02d}.hdf5')\n",
        "callback_es = EarlyStopping(monitor='val_acc', patience=10,\n",
        "                            mode='auto', verbose=1)\n",
        "\n",
        "batch_size = 50\n",
        "num_epochs = 100 # 普通は100〜300くらいを指定することが多い\n",
        "\n",
        "#h = model.fit(X_train, Y_train,\n",
        "#               batch_size=batch_size,\n",
        "#               epochs=epochs,\n",
        "#               validation_split=0.2,\n",
        "#               callbacks=[callback_op, callback_es])\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Compute quantities required for feature-wise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(X_train, Y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  epochs=num_epochs,\n",
        "                                  validation_data=(X_test, Y_test),\n",
        "                                  steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))),\n",
        "                                  workers=1)\n",
        "\n",
        "model.save('model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1000/1000 [==============================] - 128s 128ms/step - loss: 1.8378 - acc: 0.3317 - val_loss: 1.6514 - val_acc: 0.4027\n",
            "Epoch 2/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 1.5766 - acc: 0.4315 - val_loss: 1.5471 - val_acc: 0.4575\n",
            "Epoch 3/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 1.4463 - acc: 0.4786 - val_loss: 1.4587 - val_acc: 0.4782\n",
            "Epoch 4/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 1.3519 - acc: 0.5177 - val_loss: 1.4318 - val_acc: 0.5139\n",
            "Epoch 5/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 1.2770 - acc: 0.5453 - val_loss: 1.2562 - val_acc: 0.5498\n",
            "Epoch 6/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 1.2153 - acc: 0.5706 - val_loss: 1.2200 - val_acc: 0.5792\n",
            "Epoch 7/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 1.1561 - acc: 0.5917 - val_loss: 1.1348 - val_acc: 0.6070\n",
            "Epoch 8/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 1.1091 - acc: 0.6103 - val_loss: 1.1391 - val_acc: 0.6002\n",
            "Epoch 9/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 1.0676 - acc: 0.6272 - val_loss: 1.2194 - val_acc: 0.5925\n",
            "Epoch 10/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 1.0356 - acc: 0.6358 - val_loss: 1.1568 - val_acc: 0.6139\n",
            "Epoch 11/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.9996 - acc: 0.6495 - val_loss: 0.9579 - val_acc: 0.6683\n",
            "Epoch 12/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.9711 - acc: 0.6610 - val_loss: 0.9743 - val_acc: 0.6609\n",
            "Epoch 13/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.9451 - acc: 0.6694 - val_loss: 1.0076 - val_acc: 0.6629\n",
            "Epoch 14/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.9180 - acc: 0.6795 - val_loss: 0.9641 - val_acc: 0.6706\n",
            "Epoch 15/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.8959 - acc: 0.6877 - val_loss: 1.0881 - val_acc: 0.6432\n",
            "Epoch 16/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.8773 - acc: 0.6925 - val_loss: 1.0502 - val_acc: 0.6415\n",
            "Epoch 17/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.8564 - acc: 0.7015 - val_loss: 1.0164 - val_acc: 0.6746\n",
            "Epoch 18/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.8382 - acc: 0.7098 - val_loss: 0.8980 - val_acc: 0.6941\n",
            "Epoch 19/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.8195 - acc: 0.7171 - val_loss: 0.8047 - val_acc: 0.7239\n",
            "Epoch 20/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.8011 - acc: 0.7226 - val_loss: 1.0996 - val_acc: 0.6617\n",
            "Epoch 21/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 0.7814 - acc: 0.7262 - val_loss: 0.8268 - val_acc: 0.7231\n",
            "Epoch 22/100\n",
            "1000/1000 [==============================] - 123s 123ms/step - loss: 0.7697 - acc: 0.7317 - val_loss: 0.9774 - val_acc: 0.6855\n",
            "Epoch 23/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 0.7554 - acc: 0.7385 - val_loss: 0.7965 - val_acc: 0.7382\n",
            "Epoch 24/100\n",
            "1000/1000 [==============================] - 123s 123ms/step - loss: 0.7437 - acc: 0.7416 - val_loss: 0.8794 - val_acc: 0.7101\n",
            "Epoch 25/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 0.7323 - acc: 0.7468 - val_loss: 0.7971 - val_acc: 0.7371\n",
            "Epoch 26/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 0.7151 - acc: 0.7527 - val_loss: 0.8877 - val_acc: 0.7179\n",
            "Epoch 27/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 0.7064 - acc: 0.7565 - val_loss: 0.7735 - val_acc: 0.7432\n",
            "Epoch 28/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 0.6902 - acc: 0.7616 - val_loss: 0.7700 - val_acc: 0.7462\n",
            "Epoch 29/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 0.6819 - acc: 0.7657 - val_loss: 0.9046 - val_acc: 0.7204\n",
            "Epoch 30/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.6687 - acc: 0.7698 - val_loss: 0.7101 - val_acc: 0.7632\n",
            "Epoch 31/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.6573 - acc: 0.7720 - val_loss: 0.9034 - val_acc: 0.7225\n",
            "Epoch 32/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.6463 - acc: 0.7780 - val_loss: 0.7939 - val_acc: 0.7487\n",
            "Epoch 33/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 0.6430 - acc: 0.7777 - val_loss: 0.8116 - val_acc: 0.7445\n",
            "Epoch 34/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 0.6229 - acc: 0.7866 - val_loss: 0.6684 - val_acc: 0.7781\n",
            "Epoch 35/100\n",
            "1000/1000 [==============================] - 123s 123ms/step - loss: 0.6164 - acc: 0.7879 - val_loss: 0.6588 - val_acc: 0.7790\n",
            "Epoch 36/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.6133 - acc: 0.7891 - val_loss: 0.7019 - val_acc: 0.7726\n",
            "Epoch 37/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.5982 - acc: 0.7951 - val_loss: 0.7308 - val_acc: 0.7644\n",
            "Epoch 38/100\n",
            "1000/1000 [==============================] - 122s 122ms/step - loss: 0.5861 - acc: 0.7982 - val_loss: 0.6402 - val_acc: 0.7915\n",
            "Epoch 39/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.5834 - acc: 0.7983 - val_loss: 0.6377 - val_acc: 0.7905\n",
            "Epoch 40/100\n",
            "1000/1000 [==============================] - 131s 131ms/step - loss: 0.5765 - acc: 0.8034 - val_loss: 0.7115 - val_acc: 0.7745\n",
            "Epoch 41/100\n",
            "1000/1000 [==============================] - 121s 121ms/step - loss: 0.5652 - acc: 0.8069 - val_loss: 0.7597 - val_acc: 0.7631\n",
            "Epoch 42/100\n",
            "  79/1000 [=>............................] - ETA: 1:44 - loss: 0.5558 - acc: 0.8081"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-c9c9ddc7693a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                   \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                                   workers=1)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8K0-Drvzy8P5",
        "colab_type": "code",
        "outputId": "a031aa39-abff-4221-ef6d-17f5ae5bc5fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2534
        }
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# ResNet のモデルを定義するセル\n",
        "#\n",
        "\n",
        "# inputs と residual 2つの処理結果を受け取って、両者を足す\n",
        "\n",
        "# inputs：ショートカットせずに畳み込みやReLuなどをしたあとの信号\n",
        "# residual：畳み込みなどを通らずショートカットして流れてくる信号のこと、つまりモジュールに入ってくる信号と同じ\n",
        "def _shortcut(inputs, residual):\n",
        "  \n",
        "  # residual のほうのフィルタ数を取得する\n",
        "  # ちなみにデフォルトでは\n",
        "  # _keras_shape[1] 画像の幅\n",
        "  # _keras_shape[2] 画像の高さ\n",
        "  # _keras_shape[3] チャンネル数\n",
        "  # チャンネル数、幅、高さの順番のこともあるが、今回はデフォルトでOK\n",
        "  n_filters = residual._keras_shape[3]\n",
        "  \n",
        "  # inputs と residual とでチャネル数が違うかもしれない。\n",
        "  # そのままだと足せないので、1x1 conv を使って residual 側のフィルタ数に合わせている\n",
        "  shortcut = Convolution2D(n_filters, (1,1), strides=(1,1),kernel_initializer='glorot_normal', padding='same')(inputs)\n",
        " \n",
        "  # 2つを足す\n",
        "  return add([shortcut, residual])\n",
        "\n",
        "\n",
        "# ResBlock を定義\n",
        "# ここでの処理は BatchNorm → ReLU → Conv とシンプルなものにしてあるが、\n",
        "# ここを色々変更する改良案が無数にある\n",
        "###ResBlock とは、ショートカット構造のあるレイヤーの組み合わせ（ブロック）のこと\n",
        "def _resblock(n_filters, strides=(1,1)):\n",
        "  def f(inputs):  \n",
        "    x = Convolution2D(n_filters, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters, (3,3), strides=strides,kernel_initializer='he_normal', padding='same')(x)    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters * 4, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    # 入力そのものと、BN→ReLU→Conv したものとを足す\n",
        "    # この部分がResNetのもっとも重要な点\n",
        "    return _shortcut(inputs, x)\n",
        "  \n",
        "  return f\n",
        "\n",
        "def resnet():\n",
        "    inputs = Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = BatchNormalization()(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=64)(x)\n",
        "#    x = Dropout(0.4)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=64)(x)\n",
        "    \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=128)(x)\n",
        " #   x = Dropout(0.4)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        " #   x = _resblock(n_filters=128)(x)\n",
        "    \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=256)(x)\n",
        "#    x = Dropout(0.4)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "\n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=512)(x)\n",
        "#    x = Dropout(0.4)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "\n",
        "  \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(10, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "  \n",
        "model = resnet()\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_15 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 32, 32, 3)    12          input_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 32, 32, 3)    0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 32, 32, 64)   256         activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 32, 32, 64)   256         conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 32, 32, 64)   0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 32, 32, 64)   36928       activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 32, 32, 64)   256         conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 32, 32, 64)   0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 32, 32, 256)  16640       activation_198[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 32, 32, 256)  1024        conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 32, 32, 256)  1024        activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 32, 32, 256)  0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_52 (Add)                    (None, 32, 32, 256)  0           conv2d_184[0][0]                 \n",
            "                                                                 activation_199[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 32, 32, 256)  1024        add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 32, 32, 256)  0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_43 (MaxPooling2D) (None, 16, 16, 256)  0           activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 16, 16, 256)  1024        max_pooling2d_43[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 16, 16, 256)  0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 16, 16, 128)  32896       activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 16, 16, 128)  512         conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 16, 16, 128)  0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 16, 16, 128)  147584      activation_202[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 16, 16, 128)  512         conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 16, 16, 128)  0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 16, 16, 512)  66048       activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 16, 16, 512)  2048        conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 16, 16, 512)  131584      activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 16, 16, 512)  0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_53 (Add)                    (None, 16, 16, 512)  0           conv2d_188[0][0]                 \n",
            "                                                                 activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 16, 16, 512)  2048        add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 16, 16, 512)  0           batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling2D) (None, 8, 8, 512)    0           activation_205[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 8, 8, 512)    2048        max_pooling2d_44[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 8, 8, 512)    0           batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 8, 8, 256)    131328      activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 8, 8, 256)    1024        conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 8, 8, 256)    0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 8, 8, 256)    590080      activation_207[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 8, 8, 256)    1024        conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 8, 8, 256)    0           batch_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 8, 8, 1024)   263168      activation_208[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 8, 8, 1024)   4096        conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 8, 8, 1024)   525312      activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 8, 8, 1024)   0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_54 (Add)                    (None, 8, 8, 1024)   0           conv2d_192[0][0]                 \n",
            "                                                                 activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 8, 8, 1024)   4096        add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 8, 8, 1024)   0           batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling2D) (None, 4, 4, 1024)   0           activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 4, 4, 1024)   4096        max_pooling2d_45[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 4, 4, 1024)   0           batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 4, 4, 512)    524800      activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 4, 4, 512)    2048        conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 4, 4, 512)    0           batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 4, 4, 512)    2359808     activation_212[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 4, 4, 512)    2048        conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 4, 4, 512)    0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 4, 4, 2048)   1050624     activation_213[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 4, 4, 2048)   8192        conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 4, 4, 2048)   2099200     activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 4, 4, 2048)   0           batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_55 (Add)                    (None, 4, 4, 2048)   0           conv2d_196[0][0]                 \n",
            "                                                                 activation_214[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 4, 4, 2048)   8192        add_55[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 4, 4, 2048)   0           batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_13 (Gl (None, 2048)         0           activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 10)           20490       global_average_pooling2d_13[0][0]\n",
            "==================================================================================================\n",
            "Total params: 8,043,350\n",
            "Trainable params: 8,020,560\n",
            "Non-trainable params: 22,790\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bgh01dk2zjTw",
        "colab_type": "code",
        "outputId": "e430032f-45a4-4ea2-ebd1-d9648c3298cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2129
        }
      },
      "cell_type": "code",
      "source": [
        "#checkpoint\n",
        "callback_op = ModelCheckpoint(filepath='weights.{epoch:02d}.hdf5')\n",
        "callback_es = EarlyStopping(monitor='val_acc', patience=10,\n",
        "                            mode='auto', verbose=1)\n",
        "\n",
        "batch_size = 50\n",
        "num_epochs = 100 # 普通は100〜300くらいを指定することが多い\n",
        "\n",
        "#h = model.fit(X_train, Y_train,\n",
        "#               batch_size=batch_size,\n",
        "#               epochs=epochs,\n",
        "#               validation_split=0.2,\n",
        "#               callbacks=[callback_op, callback_es])\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Compute quantities required for feature-wise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(X_train, Y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  epochs=num_epochs,\n",
        "                                  validation_data=(X_test, Y_test),\n",
        "                                  steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))),\n",
        "                                  workers=1)\n",
        "\n",
        "model.save('model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1000/1000 [==============================] - 168s 168ms/step - loss: 1.8138 - acc: 0.3370 - val_loss: 1.5585 - val_acc: 0.4313\n",
            "Epoch 2/100\n",
            "1000/1000 [==============================] - 158s 158ms/step - loss: 1.5245 - acc: 0.4481 - val_loss: 1.4141 - val_acc: 0.4911\n",
            "Epoch 3/100\n",
            "1000/1000 [==============================] - 158s 158ms/step - loss: 1.3839 - acc: 0.5003 - val_loss: 1.4309 - val_acc: 0.5073\n",
            "Epoch 4/100\n",
            "1000/1000 [==============================] - 157s 157ms/step - loss: 1.2703 - acc: 0.5460 - val_loss: 1.2326 - val_acc: 0.5636\n",
            "Epoch 5/100\n",
            "1000/1000 [==============================] - 157s 157ms/step - loss: 1.1884 - acc: 0.5788 - val_loss: 1.1013 - val_acc: 0.6088\n",
            "Epoch 6/100\n",
            "1000/1000 [==============================] - 157s 157ms/step - loss: 1.1218 - acc: 0.6002 - val_loss: 1.0437 - val_acc: 0.6333\n",
            "Epoch 7/100\n",
            "1000/1000 [==============================] - 157s 157ms/step - loss: 1.0609 - acc: 0.6260 - val_loss: 1.0384 - val_acc: 0.6412\n",
            "Epoch 8/100\n",
            "1000/1000 [==============================] - 159s 159ms/step - loss: 1.0109 - acc: 0.6435 - val_loss: 0.9127 - val_acc: 0.6820\n",
            "Epoch 9/100\n",
            "1000/1000 [==============================] - 159s 159ms/step - loss: 0.9701 - acc: 0.6588 - val_loss: 0.9091 - val_acc: 0.6829\n",
            "Epoch 10/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.9324 - acc: 0.6728 - val_loss: 1.0595 - val_acc: 0.6481\n",
            "Epoch 11/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.8976 - acc: 0.6835 - val_loss: 0.8810 - val_acc: 0.6938\n",
            "Epoch 12/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.8621 - acc: 0.6967 - val_loss: 0.8670 - val_acc: 0.7075\n",
            "Epoch 13/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.8304 - acc: 0.7086 - val_loss: 0.8923 - val_acc: 0.7043\n",
            "Epoch 14/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.8052 - acc: 0.7189 - val_loss: 0.8153 - val_acc: 0.7245\n",
            "Epoch 15/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.7772 - acc: 0.7290 - val_loss: 0.7746 - val_acc: 0.7325\n",
            "Epoch 16/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.7581 - acc: 0.7338 - val_loss: 0.7375 - val_acc: 0.7471\n",
            "Epoch 17/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.7334 - acc: 0.7429 - val_loss: 0.8411 - val_acc: 0.7254\n",
            "Epoch 18/100\n",
            "1000/1000 [==============================] - 159s 159ms/step - loss: 0.7170 - acc: 0.7506 - val_loss: 0.7660 - val_acc: 0.7395\n",
            "Epoch 19/100\n",
            "1000/1000 [==============================] - 159s 159ms/step - loss: 0.6914 - acc: 0.7583 - val_loss: 0.7712 - val_acc: 0.7431\n",
            "Epoch 20/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.6722 - acc: 0.7639 - val_loss: 0.7844 - val_acc: 0.7454\n",
            "Epoch 21/100\n",
            "1000/1000 [==============================] - 159s 159ms/step - loss: 0.6489 - acc: 0.7736 - val_loss: 0.7743 - val_acc: 0.7472\n",
            "Epoch 22/100\n",
            "1000/1000 [==============================] - 159s 159ms/step - loss: 0.6387 - acc: 0.7785 - val_loss: 0.7137 - val_acc: 0.7640\n",
            "Epoch 23/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.6216 - acc: 0.7860 - val_loss: 0.7186 - val_acc: 0.7624\n",
            "Epoch 24/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.6100 - acc: 0.7873 - val_loss: 0.8025 - val_acc: 0.7520\n",
            "Epoch 25/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.5985 - acc: 0.7935 - val_loss: 0.6629 - val_acc: 0.7837\n",
            "Epoch 26/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.5789 - acc: 0.8000 - val_loss: 0.6435 - val_acc: 0.7895\n",
            "Epoch 27/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.5689 - acc: 0.8018 - val_loss: 0.6107 - val_acc: 0.8034\n",
            "Epoch 28/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.5620 - acc: 0.8047 - val_loss: 0.6398 - val_acc: 0.7883\n",
            "Epoch 29/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.5443 - acc: 0.8099 - val_loss: 0.5937 - val_acc: 0.8033\n",
            "Epoch 30/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.5325 - acc: 0.8135 - val_loss: 0.6526 - val_acc: 0.7884\n",
            "Epoch 31/100\n",
            "1000/1000 [==============================] - 162s 162ms/step - loss: 0.5212 - acc: 0.8172 - val_loss: 0.6153 - val_acc: 0.8048\n",
            "Epoch 32/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.5123 - acc: 0.8216 - val_loss: 0.6759 - val_acc: 0.7868\n",
            "Epoch 33/100\n",
            "1000/1000 [==============================] - 159s 159ms/step - loss: 0.4987 - acc: 0.8292 - val_loss: 0.6672 - val_acc: 0.7882\n",
            "Epoch 34/100\n",
            "1000/1000 [==============================] - 158s 158ms/step - loss: 0.4866 - acc: 0.8286 - val_loss: 0.6374 - val_acc: 0.7977\n",
            "Epoch 35/100\n",
            "1000/1000 [==============================] - 165s 165ms/step - loss: 0.4793 - acc: 0.8340 - val_loss: 0.5996 - val_acc: 0.8092\n",
            "Epoch 36/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.4685 - acc: 0.8366 - val_loss: 0.5940 - val_acc: 0.8101\n",
            "Epoch 37/100\n",
            "1000/1000 [==============================] - 162s 162ms/step - loss: 0.4667 - acc: 0.8393 - val_loss: 0.5794 - val_acc: 0.8151\n",
            "Epoch 38/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.4557 - acc: 0.8411 - val_loss: 0.6582 - val_acc: 0.7956\n",
            "Epoch 39/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.4491 - acc: 0.8437 - val_loss: 0.5931 - val_acc: 0.8104\n",
            "Epoch 40/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.4384 - acc: 0.8475 - val_loss: 0.5861 - val_acc: 0.8155\n",
            "Epoch 41/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.4305 - acc: 0.8512 - val_loss: 0.5682 - val_acc: 0.8192\n",
            "Epoch 42/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.4177 - acc: 0.8547 - val_loss: 0.5700 - val_acc: 0.8192\n",
            "Epoch 43/100\n",
            "1000/1000 [==============================] - 161s 161ms/step - loss: 0.4136 - acc: 0.8568 - val_loss: 0.6175 - val_acc: 0.8116\n",
            "Epoch 44/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.4087 - acc: 0.8575 - val_loss: 0.6091 - val_acc: 0.8095\n",
            "Epoch 45/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.4009 - acc: 0.8613 - val_loss: 0.5431 - val_acc: 0.8286\n",
            "Epoch 46/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.3914 - acc: 0.8650 - val_loss: 0.5756 - val_acc: 0.8189\n",
            "Epoch 47/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.3856 - acc: 0.8665 - val_loss: 0.5627 - val_acc: 0.8254\n",
            "Epoch 48/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.3791 - acc: 0.8693 - val_loss: 0.6268 - val_acc: 0.8102\n",
            "Epoch 49/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.3762 - acc: 0.8691 - val_loss: 0.5440 - val_acc: 0.8286\n",
            "Epoch 50/100\n",
            "1000/1000 [==============================] - 160s 160ms/step - loss: 0.3699 - acc: 0.8716 - val_loss: 0.5900 - val_acc: 0.8159\n",
            "Epoch 51/100\n",
            "1000/1000 [==============================] - 162s 162ms/step - loss: 0.3670 - acc: 0.8719 - val_loss: 0.6247 - val_acc: 0.8146\n",
            "Epoch 52/100\n",
            "1000/1000 [==============================] - 162s 162ms/step - loss: 0.3594 - acc: 0.8749 - val_loss: 0.5102 - val_acc: 0.8396\n",
            "Epoch 53/100\n",
            "1000/1000 [==============================] - 162s 162ms/step - loss: 0.3538 - acc: 0.8769 - val_loss: 0.5486 - val_acc: 0.8289\n",
            "Epoch 54/100\n",
            "1000/1000 [==============================] - 162s 162ms/step - loss: 0.3450 - acc: 0.8787 - val_loss: 0.6133 - val_acc: 0.8164\n",
            "Epoch 55/100\n",
            "1000/1000 [==============================] - 163s 163ms/step - loss: 0.3444 - acc: 0.8801 - val_loss: 0.5388 - val_acc: 0.8341\n",
            "Epoch 56/100\n",
            "1000/1000 [==============================] - 163s 163ms/step - loss: 0.3348 - acc: 0.8835 - val_loss: 0.5623 - val_acc: 0.8277\n",
            "Epoch 57/100\n",
            "1000/1000 [==============================] - 162s 162ms/step - loss: 0.3262 - acc: 0.8862 - val_loss: 0.5412 - val_acc: 0.8338\n",
            "Epoch 58/100\n",
            "1000/1000 [==============================] - 162s 162ms/step - loss: 0.3261 - acc: 0.8846 - val_loss: 0.6697 - val_acc: 0.8067\n",
            "Epoch 59/100\n",
            "1000/1000 [==============================] - 162s 162ms/step - loss: 0.3223 - acc: 0.8877 - val_loss: 0.5842 - val_acc: 0.8197\n",
            "Epoch 60/100\n",
            "  59/1000 [>.............................] - ETA: 2:24 - loss: 0.3049 - acc: 0.8932Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BKLU4Uhj-irF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epoch = np.arange(len(model.history['acc']))\n",
        "\n",
        "# accuracy と validation accuracy の推移をプロットする\n",
        "plt.title('Accuracy')\n",
        "plt.ylim(0.0, 1.0)\n",
        "plt.plot(epoch, model.history['acc'], label='train')\n",
        "plt.plot(epoch, model.history['val_acc'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "83iU_AbmC9YQ",
        "colab_type": "code",
        "outputId": "e129ed98-660f-46a6-8e85-d50b1564df77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2534
        }
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# ResNet のモデルを定義するセル\n",
        "#\n",
        "\n",
        "# inputs と residual 2つの処理結果を受け取って、両者を足す\n",
        "\n",
        "# inputs：ショートカットせずに畳み込みやReLuなどをしたあとの信号\n",
        "# residual：畳み込みなどを通らずショートカットして流れてくる信号のこと、つまりモジュールに入ってくる信号と同じ\n",
        "def _shortcut(inputs, residual):\n",
        "  \n",
        "  # residual のほうのフィルタ数を取得する\n",
        "  # ちなみにデフォルトでは\n",
        "  # _keras_shape[1] 画像の幅\n",
        "  # _keras_shape[2] 画像の高さ\n",
        "  # _keras_shape[3] チャンネル数\n",
        "  # チャンネル数、幅、高さの順番のこともあるが、今回はデフォルトでOK\n",
        "  n_filters = residual._keras_shape[3]\n",
        "  \n",
        "  # inputs と residual とでチャネル数が違うかもしれない。\n",
        "  # そのままだと足せないので、1x1 conv を使って residual 側のフィルタ数に合わせている\n",
        "  shortcut = Convolution2D(n_filters, (1,1), strides=(1,1),kernel_initializer='glorot_normal', padding='same')(inputs)\n",
        " \n",
        "  # 2つを足す\n",
        "  return add([shortcut, residual])\n",
        "\n",
        "\n",
        "# ResBlock を定義\n",
        "# ここでの処理は BatchNorm → ReLU → Conv とシンプルなものにしてあるが、\n",
        "# ここを色々変更する改良案が無数にある\n",
        "###ResBlock とは、ショートカット構造のあるレイヤーの組み合わせ（ブロック）のこと\n",
        "def _resblock(n_filters, strides=(1,1)):\n",
        "  def f(inputs):  \n",
        "    x = Convolution2D(n_filters, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters, (3,3), strides=strides,kernel_initializer='he_normal', padding='same')(x)    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters * 4, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    # 入力そのものと、BN→ReLU→Conv したものとを足す\n",
        "    # この部分がResNetのもっとも重要な点\n",
        "    return _shortcut(inputs, x)\n",
        "  \n",
        "  return f\n",
        "\n",
        "def resnet():\n",
        "    inputs = Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = BatchNormalization()(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=64)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=64)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "#    x = Dropout(0.2)(x)\n",
        "  \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=128)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=128)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "    \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "  \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(10, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "  \n",
        "model = resnet()\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 32, 32, 3)    12          input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 32, 32, 3)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 32, 32, 64)   256         activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 32, 32, 64)   256         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 32, 32, 64)   0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 32, 32, 64)   36928       activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 32, 32, 64)   256         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 32, 32, 64)   0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 32, 32, 256)  16640       activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 32, 32, 256)  1024        conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 32, 32, 256)  1024        activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 32, 32, 256)  0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 32, 32, 256)  0           conv2d_68[0][0]                  \n",
            "                                                                 activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 32, 32, 256)  1024        add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 32, 32, 256)  0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 16, 16, 256)  0           activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 16, 16, 256)  1024        max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 16, 16, 256)  0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 16, 16, 128)  32896       activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 16, 16, 128)  512         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 16, 16, 128)  0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 16, 16, 128)  147584      activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 16, 16, 128)  512         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 16, 16, 128)  0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 16, 16, 512)  66048       activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 16, 16, 512)  2048        conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 16, 16, 512)  131584      activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 16, 16, 512)  0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 16, 16, 512)  0           conv2d_72[0][0]                  \n",
            "                                                                 activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 16, 16, 512)  2048        add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 16, 16, 512)  0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 8, 8, 512)    0           activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 8, 8, 512)    2048        max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 8, 8, 512)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 8, 8, 256)    131328      activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 8, 8, 256)    1024        conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 8, 8, 256)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 8, 8, 256)    590080      activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 8, 8, 256)    1024        conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 8, 8, 256)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 8, 8, 1024)   263168      activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 8, 8, 1024)   4096        conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 8, 8, 1024)   525312      activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 8, 8, 1024)   0           conv2d_76[0][0]                  \n",
            "                                                                 activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 8, 8, 1024)   4096        add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 4, 4, 1024)   0           activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 4, 4, 1024)   4096        max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 4, 4, 512)    524800      activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 4, 4, 512)    2048        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 4, 4, 512)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 4, 4, 512)    2359808     activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 4, 4, 512)    2048        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 4, 4, 512)    0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 4, 4, 2048)   1050624     activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 4, 4, 2048)   8192        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 4, 4, 2048)   2099200     activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 4, 4, 2048)   0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 4, 4, 2048)   0           conv2d_80[0][0]                  \n",
            "                                                                 activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 4, 4, 2048)   8192        add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 4, 4, 2048)   0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 2048)         0           activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 10)           20490       global_average_pooling2d_4[0][0] \n",
            "==================================================================================================\n",
            "Total params: 8,043,350\n",
            "Trainable params: 8,020,560\n",
            "Non-trainable params: 22,790\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IYSfJGO5nmUP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# ResNet のモデルを定義するセル\n",
        "#\n",
        "\n",
        "# inputs と residual 2つの処理結果を受け取って、両者を足す\n",
        "\n",
        "# inputs：ショートカットせずに畳み込みやReLuなどをしたあとの信号\n",
        "# residual：畳み込みなどを通らずショートカットして流れてくる信号のこと、つまりモジュールに入ってくる信号と同じ\n",
        "def _shortcut(inputs, residual):\n",
        "  \n",
        "  # residual のほうのフィルタ数を取得する\n",
        "  # ちなみにデフォルトでは\n",
        "  # _keras_shape[1] 画像の幅\n",
        "  # _keras_shape[2] 画像の高さ\n",
        "  # _keras_shape[3] チャンネル数\n",
        "  # チャンネル数、幅、高さの順番のこともあるが、今回はデフォルトでOK\n",
        "  n_filters = residual._keras_shape[3]\n",
        "  \n",
        "  # inputs と residual とでチャネル数が違うかもしれない。\n",
        "  # そのままだと足せないので、1x1 conv を使って residual 側のフィルタ数に合わせている\n",
        "  shortcut = Convolution2D(n_filters, (1,1), strides=(1,1),kernel_initializer='glorot_normal', padding='same')(inputs)\n",
        " \n",
        "  # 2つを足す\n",
        "  return add([shortcut, residual])\n",
        "\n",
        "\n",
        "# ResBlock を定義\n",
        "# ここでの処理は BatchNorm → ReLU → Conv とシンプルなものにしてあるが、\n",
        "# ここを色々変更する改良案が無数にある\n",
        "###ResBlock とは、ショートカット構造のあるレイヤーの組み合わせ（ブロック）のこと\n",
        "def _resblock(n_filters, strides=(1,1)):\n",
        "  def f(inputs):  \n",
        "    x = Convolution2D(n_filters, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters, (3,3), strides=strides,kernel_initializer='he_normal', padding='same')(x)    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters * 4, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    # 入力そのものと、BN→ReLU→Conv したものとを足す\n",
        "    # この部分がResNetのもっとも重要な点\n",
        "    return _shortcut(inputs, x)\n",
        "  \n",
        "  return f\n",
        "\n",
        "def resnet():\n",
        "    inputs = Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = BatchNormalization()(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=64)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=64)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "#    x = Dropout(0.2)(x)\n",
        "  \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=128)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=128)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "    \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "  \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(10, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "  \n",
        "model = resnet()\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p6YSl8UxDBQ-",
        "colab_type": "code",
        "outputId": "c6314d07-e578-45a9-d8a2-fb0be81cd09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2833
        }
      },
      "cell_type": "code",
      "source": [
        "#checkpoint\n",
        "#callback_op = ModelCheckpoint(filepath='weights.{epoch:02d}.hdf5')\n",
        "callback_es = EarlyStopping(monitor='val_acc', patience=10,\n",
        "                            mode='auto', verbose=1)\n",
        "\n",
        "batch_size = 30\n",
        "num_epochs = 100 # 普通は100〜300くらいを指定することが多い\n",
        "\n",
        "#h = model.fit(X_train, Y_train,\n",
        "#               batch_size=batch_size,\n",
        "#               epochs=epochs,\n",
        "#               validation_split=0.2,\n",
        "#               callbacks=[callback_op, callback_es])\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Compute quantities required for feature-wise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(X_train, Y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  epochs=num_epochs,\n",
        "                                  validation_data=(X_test, Y_test),\n",
        "                                  steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))),\n",
        "                                  workers=1)\n",
        "\n",
        "model.save('model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1667/1667 [==============================] - 213s 128ms/step - loss: 1.7671 - acc: 0.3513 - val_loss: 1.6437 - val_acc: 0.4352\n",
            "Epoch 2/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 1.4559 - acc: 0.4759 - val_loss: 1.3901 - val_acc: 0.5157\n",
            "Epoch 3/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 1.2916 - acc: 0.5388 - val_loss: 1.2000 - val_acc: 0.5794\n",
            "Epoch 4/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 1.1803 - acc: 0.5790 - val_loss: 1.1703 - val_acc: 0.5924\n",
            "Epoch 5/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 1.0930 - acc: 0.6120 - val_loss: 1.0443 - val_acc: 0.6483\n",
            "Epoch 6/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 1.0208 - acc: 0.6409 - val_loss: 0.9213 - val_acc: 0.6759\n",
            "Epoch 7/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.9643 - acc: 0.6611 - val_loss: 0.9968 - val_acc: 0.6620\n",
            "Epoch 8/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.9122 - acc: 0.6802 - val_loss: 0.9135 - val_acc: 0.6867\n",
            "Epoch 9/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.8660 - acc: 0.6963 - val_loss: 0.8796 - val_acc: 0.7046\n",
            "Epoch 10/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.8219 - acc: 0.7140 - val_loss: 0.9029 - val_acc: 0.7068\n",
            "Epoch 11/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.7879 - acc: 0.7237 - val_loss: 0.6987 - val_acc: 0.7598\n",
            "Epoch 12/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.7561 - acc: 0.7382 - val_loss: 0.7655 - val_acc: 0.7397\n",
            "Epoch 13/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.7290 - acc: 0.7473 - val_loss: 0.7655 - val_acc: 0.7393\n",
            "Epoch 14/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.7039 - acc: 0.7550 - val_loss: 0.7830 - val_acc: 0.7407\n",
            "Epoch 15/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.6797 - acc: 0.7643 - val_loss: 0.6900 - val_acc: 0.7648\n",
            "Epoch 16/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.6533 - acc: 0.7746 - val_loss: 0.6782 - val_acc: 0.7756\n",
            "Epoch 17/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.6318 - acc: 0.7806 - val_loss: 0.7258 - val_acc: 0.7610\n",
            "Epoch 18/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.6168 - acc: 0.7856 - val_loss: 0.7924 - val_acc: 0.7537\n",
            "Epoch 19/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.5959 - acc: 0.7937 - val_loss: 0.5734 - val_acc: 0.8053\n",
            "Epoch 20/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.5811 - acc: 0.7980 - val_loss: 0.7157 - val_acc: 0.7743\n",
            "Epoch 21/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.5623 - acc: 0.8049 - val_loss: 0.6259 - val_acc: 0.7909\n",
            "Epoch 22/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.5480 - acc: 0.8081 - val_loss: 0.6417 - val_acc: 0.7918\n",
            "Epoch 23/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.5315 - acc: 0.8144 - val_loss: 0.6623 - val_acc: 0.7899\n",
            "Epoch 24/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.5177 - acc: 0.8182 - val_loss: 0.5550 - val_acc: 0.8167\n",
            "Epoch 25/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.5076 - acc: 0.8230 - val_loss: 0.5936 - val_acc: 0.8111\n",
            "Epoch 26/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.4972 - acc: 0.8235 - val_loss: 0.5095 - val_acc: 0.8331\n",
            "Epoch 27/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.4800 - acc: 0.8337 - val_loss: 0.6029 - val_acc: 0.8092\n",
            "Epoch 28/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.4718 - acc: 0.8373 - val_loss: 0.4943 - val_acc: 0.8337\n",
            "Epoch 29/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.4586 - acc: 0.8412 - val_loss: 0.5332 - val_acc: 0.8236\n",
            "Epoch 30/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.4492 - acc: 0.8452 - val_loss: 0.6213 - val_acc: 0.8114\n",
            "Epoch 31/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.4438 - acc: 0.8465 - val_loss: 0.5571 - val_acc: 0.8255\n",
            "Epoch 32/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.4328 - acc: 0.8503 - val_loss: 0.5531 - val_acc: 0.8254\n",
            "Epoch 33/100\n",
            "1667/1667 [==============================] - 206s 123ms/step - loss: 0.4149 - acc: 0.8562 - val_loss: 0.5246 - val_acc: 0.8341\n",
            "Epoch 34/100\n",
            "1667/1667 [==============================] - 206s 124ms/step - loss: 0.4111 - acc: 0.8556 - val_loss: 0.5270 - val_acc: 0.8328\n",
            "Epoch 35/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.4024 - acc: 0.8603 - val_loss: 0.5015 - val_acc: 0.8377\n",
            "Epoch 36/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.3917 - acc: 0.8643 - val_loss: 0.5237 - val_acc: 0.8413\n",
            "Epoch 37/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.3882 - acc: 0.8638 - val_loss: 0.4656 - val_acc: 0.8504\n",
            "Epoch 38/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.3850 - acc: 0.8660 - val_loss: 0.5724 - val_acc: 0.8234\n",
            "Epoch 39/100\n",
            "1667/1667 [==============================] - 207s 124ms/step - loss: 0.3686 - acc: 0.8713 - val_loss: 0.5281 - val_acc: 0.8381\n",
            "Epoch 40/100\n",
            "1667/1667 [==============================] - 204s 123ms/step - loss: 0.3564 - acc: 0.8747 - val_loss: 0.6150 - val_acc: 0.8152\n",
            "Epoch 41/100\n",
            "1667/1667 [==============================] - 204s 122ms/step - loss: 0.3544 - acc: 0.8766 - val_loss: 0.4762 - val_acc: 0.8512\n",
            "Epoch 42/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.3484 - acc: 0.8785 - val_loss: 0.5747 - val_acc: 0.8238\n",
            "Epoch 43/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.3392 - acc: 0.8814 - val_loss: 0.4931 - val_acc: 0.8430\n",
            "Epoch 44/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.3346 - acc: 0.8833 - val_loss: 0.5385 - val_acc: 0.8348\n",
            "Epoch 45/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.3240 - acc: 0.8872 - val_loss: 0.6407 - val_acc: 0.8134\n",
            "Epoch 46/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.3304 - acc: 0.8837 - val_loss: 0.5060 - val_acc: 0.8496\n",
            "Epoch 47/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.3213 - acc: 0.8887 - val_loss: 0.4881 - val_acc: 0.8473\n",
            "Epoch 48/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.3175 - acc: 0.8903 - val_loss: 0.5676 - val_acc: 0.8300\n",
            "Epoch 49/100\n",
            "1667/1667 [==============================] - 204s 123ms/step - loss: 0.3061 - acc: 0.8927 - val_loss: 0.5194 - val_acc: 0.8388\n",
            "Epoch 50/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.3000 - acc: 0.8965 - val_loss: 0.4807 - val_acc: 0.8524\n",
            "Epoch 51/100\n",
            "1667/1667 [==============================] - 204s 122ms/step - loss: 0.2967 - acc: 0.8968 - val_loss: 0.4502 - val_acc: 0.8598\n",
            "Epoch 52/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.2936 - acc: 0.8980 - val_loss: 0.4785 - val_acc: 0.8562\n",
            "Epoch 53/100\n",
            "1667/1667 [==============================] - 204s 123ms/step - loss: 0.2906 - acc: 0.8984 - val_loss: 0.4690 - val_acc: 0.8565\n",
            "Epoch 54/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.2837 - acc: 0.9017 - val_loss: 0.4780 - val_acc: 0.8568\n",
            "Epoch 55/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.2755 - acc: 0.9038 - val_loss: 0.4431 - val_acc: 0.8640\n",
            "Epoch 56/100\n",
            "1667/1667 [==============================] - 206s 123ms/step - loss: 0.2713 - acc: 0.9054 - val_loss: 0.4784 - val_acc: 0.8546\n",
            "Epoch 57/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.2679 - acc: 0.9075 - val_loss: 0.4819 - val_acc: 0.8592\n",
            "Epoch 58/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.2585 - acc: 0.9097 - val_loss: 0.5039 - val_acc: 0.8506\n",
            "Epoch 59/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.2571 - acc: 0.9107 - val_loss: 0.4923 - val_acc: 0.8576\n",
            "Epoch 60/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.2534 - acc: 0.9126 - val_loss: 0.4995 - val_acc: 0.8563\n",
            "Epoch 61/100\n",
            "1667/1667 [==============================] - 206s 123ms/step - loss: 0.2541 - acc: 0.9117 - val_loss: 0.4717 - val_acc: 0.8589\n",
            "Epoch 62/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.2443 - acc: 0.9147 - val_loss: 0.4681 - val_acc: 0.8610\n",
            "Epoch 63/100\n",
            "1667/1667 [==============================] - 204s 123ms/step - loss: 0.2437 - acc: 0.9155 - val_loss: 0.4863 - val_acc: 0.8554\n",
            "Epoch 64/100\n",
            "1667/1667 [==============================] - 205s 123ms/step - loss: 0.2440 - acc: 0.9134 - val_loss: 0.4470 - val_acc: 0.8664\n",
            "Epoch 65/100\n",
            "1667/1667 [==============================] - 204s 123ms/step - loss: 0.2325 - acc: 0.9188 - val_loss: 0.4581 - val_acc: 0.8679\n",
            "Epoch 66/100\n",
            "1667/1667 [==============================] - 203s 122ms/step - loss: 0.2278 - acc: 0.9186 - val_loss: 0.4781 - val_acc: 0.8599\n",
            "Epoch 67/100\n",
            "1667/1667 [==============================] - 203s 122ms/step - loss: 0.2287 - acc: 0.9196 - val_loss: 0.5456 - val_acc: 0.8483\n",
            "Epoch 68/100\n",
            "1667/1667 [==============================] - 204s 122ms/step - loss: 0.2229 - acc: 0.9215 - val_loss: 0.4639 - val_acc: 0.8658\n",
            "Epoch 69/100\n",
            "1667/1667 [==============================] - 203s 122ms/step - loss: 0.2192 - acc: 0.9223 - val_loss: 0.4786 - val_acc: 0.8622\n",
            "Epoch 70/100\n",
            "1667/1667 [==============================] - 203s 122ms/step - loss: 0.2165 - acc: 0.9252 - val_loss: 0.5110 - val_acc: 0.8587\n",
            "Epoch 71/100\n",
            "1667/1667 [==============================] - 203s 122ms/step - loss: 0.2155 - acc: 0.9252 - val_loss: 0.4630 - val_acc: 0.8697\n",
            "Epoch 72/100\n",
            "1667/1667 [==============================] - 203s 122ms/step - loss: 0.2075 - acc: 0.9276 - val_loss: 0.4665 - val_acc: 0.8639\n",
            "Epoch 73/100\n",
            "1667/1667 [==============================] - 204s 122ms/step - loss: 0.2101 - acc: 0.9266 - val_loss: 0.4650 - val_acc: 0.8651\n",
            "Epoch 74/100\n",
            "1667/1667 [==============================] - 204s 122ms/step - loss: 0.2067 - acc: 0.9284 - val_loss: 0.4623 - val_acc: 0.8652\n",
            "Epoch 75/100\n",
            "1667/1667 [==============================] - 203s 122ms/step - loss: 0.1981 - acc: 0.9307 - val_loss: 0.5061 - val_acc: 0.8563\n",
            "Epoch 76/100\n",
            "1667/1667 [==============================] - 203s 122ms/step - loss: 0.1946 - acc: 0.9324 - val_loss: 0.4469 - val_acc: 0.8733\n",
            "Epoch 77/100\n",
            "1667/1667 [==============================] - 203s 122ms/step - loss: 0.1968 - acc: 0.9304 - val_loss: 0.5352 - val_acc: 0.8550\n",
            "Epoch 78/100\n",
            "1667/1667 [==============================] - 203s 122ms/step - loss: 0.1895 - acc: 0.9335 - val_loss: 0.4652 - val_acc: 0.8711\n",
            "Epoch 79/100\n",
            "1667/1667 [==============================] - 204s 122ms/step - loss: 0.1897 - acc: 0.9349 - val_loss: 0.4633 - val_acc: 0.8683\n",
            "Epoch 80/100\n",
            "1023/1667 [=================>............] - ETA: 1:13 - loss: 0.1853 - acc: 0.9350"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TVYDUHovaUQo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epoch = np.arange(len(h.history['acc']))\n",
        "\n",
        "# accuracy と validation accuracy の推移をプロットする\n",
        "plt.title('Accuracy')\n",
        "plt.ylim(0.0, 1.0)\n",
        "plt.plot(epoch, h.history['acc'], label='train')\n",
        "plt.plot(epoch, h.history['val_acc'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3YY-g2PVPKtZ",
        "colab_type": "code",
        "outputId": "52b66e3d-1fef-44fc-c778-0585b3e2eaed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2815
        }
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# ResNet のモデルを定義するセル\n",
        "#\n",
        "\n",
        "# inputs と residual 2つの処理結果を受け取って、両者を足す\n",
        "\n",
        "# inputs：ショートカットせずに畳み込みやReLuなどをしたあとの信号\n",
        "# residual：畳み込みなどを通らずショートカットして流れてくる信号のこと、つまりモジュールに入ってくる信号と同じ\n",
        "def _shortcut(inputs, residual):\n",
        "  \n",
        "  # residual のほうのフィルタ数を取得する\n",
        "  # ちなみにデフォルトでは\n",
        "  # _keras_shape[1] 画像の幅\n",
        "  # _keras_shape[2] 画像の高さ\n",
        "  # _keras_shape[3] チャンネル数\n",
        "  # チャンネル数、幅、高さの順番のこともあるが、今回はデフォルトでOK\n",
        "  n_filters = residual._keras_shape[3]\n",
        "  \n",
        "  # inputs と residual とでチャネル数が違うかもしれない。\n",
        "  # そのままだと足せないので、1x1 conv を使って residual 側のフィルタ数に合わせている\n",
        "  shortcut = Convolution2D(n_filters, (1,1), strides=(1,1),kernel_initializer='glorot_normal', padding='same')(inputs)\n",
        " \n",
        "  # 2つを足す\n",
        "  return add([shortcut, residual])\n",
        "\n",
        "\n",
        "# ResBlock を定義\n",
        "# ここでの処理は BatchNorm → ReLU → Conv とシンプルなものにしてあるが、\n",
        "# ここを色々変更する改良案が無数にある\n",
        "###ResBlock とは、ショートカット構造のあるレイヤーの組み合わせ（ブロック）のこと\n",
        "def _resblock(n_filters, strides=(1,1)):\n",
        "  def f(inputs):  \n",
        "    \n",
        "    x = BatchNormalization()(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    x = Convolution2D(n_filters, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters, (3,3), strides=strides,kernel_initializer='he_normal', padding='same')(x)    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters * 4, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "  #  x = Dropout(0.3)(x)\n",
        "    \n",
        "    # 入力そのものと、BN→ReLU→Conv したものとを足す\n",
        "    # この部分がResNetのもっとも重要な点\n",
        "    return _shortcut(inputs, x)\n",
        "  \n",
        "  return f\n",
        "\n",
        "def resnet():\n",
        "    inputs = Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = BatchNormalization()(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=64)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=64)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "#    x = Dropout(0.2)(x)\n",
        "  \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=128)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=128)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "    \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "  \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(10, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "  \n",
        "model = resnet()\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 32, 32, 3)    12          input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 32, 32, 3)    0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 32, 32, 3)    12          activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 32, 32, 3)    0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 32, 32, 64)   256         activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 32, 32, 64)   256         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 32, 32, 64)   0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 32, 32, 64)   36928       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 32, 32, 64)   256         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 32, 32, 64)   0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 32, 32, 256)  16640       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 32, 32, 256)  1024        conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 32, 32, 256)  1024        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 32, 32, 256)  0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 32, 32, 256)  0           conv2d_36[0][0]                  \n",
            "                                                                 activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 32, 32, 256)  1024        add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 32, 32, 256)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 16, 16, 256)  1024        max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 16, 16, 256)  0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 16, 16, 256)  1024        activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 16, 16, 256)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 16, 16, 128)  32896       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 16, 16, 128)  512         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 16, 16, 128)  0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 16, 16, 128)  147584      activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 16, 16, 128)  512         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 16, 16, 128)  0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 16, 16, 512)  66048       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 16, 16, 512)  2048        conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 16, 16, 512)  131584      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 16, 16, 512)  0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 16, 16, 512)  0           conv2d_40[0][0]                  \n",
            "                                                                 activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 16, 16, 512)  2048        add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 16, 16, 512)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 8, 8, 512)    0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 8, 8, 512)    2048        max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 8, 8, 512)    0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 8, 8, 512)    2048        activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 8, 8, 512)    0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 8, 8, 256)    131328      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 8, 8, 256)    1024        conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 8, 8, 256)    0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 8, 8, 256)    590080      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 8, 8, 256)    1024        conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 8, 8, 256)    0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 8, 8, 1024)   263168      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 8, 8, 1024)   4096        conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 8, 8, 1024)   525312      activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 8, 8, 1024)   0           conv2d_44[0][0]                  \n",
            "                                                                 activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 8, 8, 1024)   4096        add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 4, 4, 1024)   0           activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 4, 4, 1024)   4096        max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 4, 4, 1024)   4096        activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 4, 4, 512)    524800      activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 4, 4, 512)    2048        conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 4, 4, 512)    0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 512)    2359808     activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 4, 4, 512)    2048        conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 4, 4, 512)    0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 2048)   1050624     activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 4, 4, 2048)   8192        conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 2048)   2099200     activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 4, 4, 2048)   0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 4, 4, 2048)   0           conv2d_48[0][0]                  \n",
            "                                                                 activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 4, 4, 2048)   8192        add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 4, 4, 2048)   0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 2048)         0           activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           20490       global_average_pooling2d_3[0][0] \n",
            "==================================================================================================\n",
            "Total params: 8,050,530\n",
            "Trainable params: 8,024,150\n",
            "Non-trainable params: 26,380\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-B5RXL3fPZH3",
        "colab_type": "code",
        "outputId": "7520fde5-d3a5-44c0-8d29-420fa943ca95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1214
        }
      },
      "cell_type": "code",
      "source": [
        "#checkpoint\n",
        "#callback_op = ModelCheckpoint(filepath='weights.{epoch:02d}.hdf5')\n",
        "callback_es = EarlyStopping(monitor='val_acc', patience=10,\n",
        "                            mode='auto', verbose=1)\n",
        "\n",
        "batch_size = 30\n",
        "num_epochs = 100 # 普通は100〜300くらいを指定することが多い\n",
        "\n",
        "#h = model.fit(X_train, Y_train,\n",
        "#               batch_size=batch_size,\n",
        "#               epochs=epochs,\n",
        "#               validation_split=0.2,\n",
        "#               callbacks=[callback_op, callback_es])\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Compute quantities required for feature-wise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(X_train, Y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  epochs=num_epochs,\n",
        "                                  validation_data=(X_test, Y_test),\n",
        "                                  steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))),\n",
        "                                  workers=1)\n",
        "\n",
        "model.save('model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1667/1667 [==============================] - 219s 131ms/step - loss: 1.8476 - acc: 0.3225 - val_loss: 1.5846 - val_acc: 0.4303\n",
            "Epoch 2/100\n",
            "1667/1667 [==============================] - 214s 128ms/step - loss: 1.5402 - acc: 0.4415 - val_loss: 1.3956 - val_acc: 0.4992\n",
            "Epoch 3/100\n",
            "1667/1667 [==============================] - 214s 129ms/step - loss: 1.3754 - acc: 0.5067 - val_loss: 1.3202 - val_acc: 0.5406\n",
            "Epoch 4/100\n",
            "1667/1667 [==============================] - 215s 129ms/step - loss: 1.2529 - acc: 0.5548 - val_loss: 1.1222 - val_acc: 0.5982\n",
            "Epoch 5/100\n",
            "1667/1667 [==============================] - 213s 128ms/step - loss: 1.1626 - acc: 0.5866 - val_loss: 1.1120 - val_acc: 0.6103\n",
            "Epoch 6/100\n",
            "1667/1667 [==============================] - 215s 129ms/step - loss: 1.0827 - acc: 0.6173 - val_loss: 1.0794 - val_acc: 0.6338\n",
            "Epoch 7/100\n",
            "1667/1667 [==============================] - 216s 129ms/step - loss: 1.0187 - acc: 0.6395 - val_loss: 1.0293 - val_acc: 0.6502\n",
            "Epoch 8/100\n",
            "1667/1667 [==============================] - 215s 129ms/step - loss: 0.9678 - acc: 0.6602 - val_loss: 1.0463 - val_acc: 0.6447\n",
            "Epoch 9/100\n",
            "1667/1667 [==============================] - 215s 129ms/step - loss: 0.9146 - acc: 0.6794 - val_loss: 0.9815 - val_acc: 0.6787\n",
            "Epoch 10/100\n",
            "1667/1667 [==============================] - 215s 129ms/step - loss: 0.8752 - acc: 0.6957 - val_loss: 0.9177 - val_acc: 0.6964\n",
            "Epoch 11/100\n",
            "1667/1667 [==============================] - 215s 129ms/step - loss: 0.8376 - acc: 0.7098 - val_loss: 0.8314 - val_acc: 0.7257\n",
            "Epoch 12/100\n",
            "1667/1667 [==============================] - 215s 129ms/step - loss: 0.7977 - acc: 0.7214 - val_loss: 1.0576 - val_acc: 0.6711\n",
            "Epoch 13/100\n",
            "1667/1667 [==============================] - 217s 130ms/step - loss: 0.7670 - acc: 0.7311 - val_loss: 0.8867 - val_acc: 0.7184\n",
            "Epoch 14/100\n",
            "1667/1667 [==============================] - 215s 129ms/step - loss: 0.7460 - acc: 0.7404 - val_loss: 0.7664 - val_acc: 0.7478\n",
            "Epoch 15/100\n",
            "1667/1667 [==============================] - 241s 145ms/step - loss: 0.7174 - acc: 0.7492 - val_loss: 0.7283 - val_acc: 0.7599\n",
            "Epoch 16/100\n",
            "1667/1667 [==============================] - 214s 128ms/step - loss: 0.6943 - acc: 0.7582 - val_loss: 0.7794 - val_acc: 0.7437\n",
            "Epoch 17/100\n",
            "1667/1667 [==============================] - 214s 128ms/step - loss: 0.6727 - acc: 0.7653 - val_loss: 0.7602 - val_acc: 0.7544\n",
            "Epoch 18/100\n",
            "1667/1667 [==============================] - 214s 128ms/step - loss: 0.6511 - acc: 0.7733 - val_loss: 0.8223 - val_acc: 0.7426\n",
            "Epoch 19/100\n",
            "1667/1667 [==============================] - 214s 129ms/step - loss: 0.6356 - acc: 0.7797 - val_loss: 0.7495 - val_acc: 0.7584\n",
            "Epoch 20/100\n",
            "1667/1667 [==============================] - 215s 129ms/step - loss: 0.6150 - acc: 0.7855 - val_loss: 0.8179 - val_acc: 0.7396\n",
            "Epoch 21/100\n",
            "1667/1667 [==============================] - 215s 129ms/step - loss: 0.5966 - acc: 0.7920 - val_loss: 0.6695 - val_acc: 0.7795\n",
            "Epoch 22/100\n",
            "1667/1667 [==============================] - 215s 129ms/step - loss: 0.5824 - acc: 0.7974 - val_loss: 0.6224 - val_acc: 0.7938\n",
            "Epoch 23/100\n",
            "1667/1667 [==============================] - 225s 135ms/step - loss: 0.5684 - acc: 0.8021 - val_loss: 0.6051 - val_acc: 0.7959\n",
            "Epoch 24/100\n",
            "1667/1667 [==============================] - 217s 130ms/step - loss: 0.5485 - acc: 0.8098 - val_loss: 0.5631 - val_acc: 0.8130\n",
            "Epoch 25/100\n",
            "1667/1667 [==============================] - 218s 131ms/step - loss: 0.5391 - acc: 0.8133 - val_loss: 0.6787 - val_acc: 0.7898\n",
            "Epoch 26/100\n",
            "1667/1667 [==============================] - 214s 129ms/step - loss: 0.5288 - acc: 0.8136 - val_loss: 0.7167 - val_acc: 0.7753\n",
            "Epoch 27/100\n",
            "1667/1667 [==============================] - 214s 129ms/step - loss: 0.5109 - acc: 0.8236 - val_loss: 0.7392 - val_acc: 0.7698\n",
            "Epoch 28/100\n",
            "1667/1667 [==============================] - 215s 129ms/step - loss: 0.4950 - acc: 0.8272 - val_loss: 0.5823 - val_acc: 0.8117\n",
            "Epoch 29/100\n",
            "1667/1667 [==============================] - 214s 129ms/step - loss: 0.4819 - acc: 0.8335 - val_loss: 0.5921 - val_acc: 0.8098\n",
            "Epoch 30/100\n",
            "1667/1667 [==============================] - 217s 130ms/step - loss: 0.4829 - acc: 0.8307 - val_loss: 0.6683 - val_acc: 0.7927\n",
            "Epoch 31/100\n",
            "1667/1667 [==============================] - 272s 163ms/step - loss: 0.4669 - acc: 0.8368 - val_loss: 0.5926 - val_acc: 0.8099\n",
            "Epoch 32/100\n",
            "1667/1667 [==============================] - 275s 165ms/step - loss: 0.4538 - acc: 0.8418 - val_loss: 0.5353 - val_acc: 0.8250\n",
            "Epoch 33/100\n",
            "1667/1667 [==============================] - 217s 130ms/step - loss: 0.4463 - acc: 0.8450 - val_loss: 0.5166 - val_acc: 0.8297\n",
            "Epoch 34/100\n",
            " 213/1667 [==>...........................] - ETA: 2:55 - loss: 0.4363 - acc: 0.8477"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MfnYOkvwHGlG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "オプティマイザを変更したら精度向上\n",
        "\n",
        "opt = Adam(amsgrad=True)  \n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "metadata": {
        "id": "KrVgTYdanvvV",
        "colab_type": "code",
        "outputId": "66ff564d-fead-4520-b6bd-3a41d6c805fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1601
        }
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# ResNet のモデルを定義するセル\n",
        "#\n",
        "\n",
        "# inputs と residual 2つの処理結果を受け取って、両者を足す\n",
        "\n",
        "# inputs：ショートカットせずに畳み込みやReLuなどをしたあとの信号\n",
        "# residual：畳み込みなどを通らずショートカットして流れてくる信号のこと、つまりモジュールに入ってくる信号と同じ\n",
        "def _shortcut(inputs, residual):\n",
        "  \n",
        "  # residual のほうのフィルタ数を取得する\n",
        "  # ちなみにデフォルトでは\n",
        "  # _keras_shape[1] 画像の幅\n",
        "  # _keras_shape[2] 画像の高さ\n",
        "  # _keras_shape[3] チャンネル数\n",
        "  # チャンネル数、幅、高さの順番のこともあるが、今回はデフォルトでOK\n",
        "  n_filters = residual._keras_shape[3]\n",
        "  \n",
        "  # inputs と residual とでチャネル数が違うかもしれない。\n",
        "  # そのままだと足せないので、1x1 conv を使って residual 側のフィルタ数に合わせている\n",
        "  shortcut = Convolution2D(n_filters, (1,1), strides=(1,1),kernel_initializer='glorot_normal', padding='same')(inputs)\n",
        " \n",
        "  # 2つを足す\n",
        "  return add([shortcut, residual])\n",
        "\n",
        "\n",
        "# ResBlock を定義\n",
        "# ここでの処理は BatchNorm → ReLU → Conv とシンプルなものにしてあるが、\n",
        "# ここを色々変更する改良案が無数にある\n",
        "###ResBlock とは、ショートカット構造のあるレイヤーの組み合わせ（ブロック）のこと\n",
        "def _resblock(n_filters, strides=(1,1)):\n",
        "  def f(inputs):  \n",
        "    x = Convolution2D(n_filters, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters, (3,3), strides=strides,kernel_initializer='he_normal', padding='same')(x)    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters * 4, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    # 入力そのものと、BN→ReLU→Conv したものとを足す\n",
        "    # この部分がResNetのもっとも重要な点\n",
        "    return _shortcut(inputs, x)\n",
        "  \n",
        "  return f\n",
        "\n",
        "def resnet():\n",
        "    inputs = Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = BatchNormalization()(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=64)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=64)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "#    x = Dropout(0.2)(x)\n",
        "  \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=128)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=128)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "    \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "  \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(10, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "  \n",
        "opt = Adam(amsgrad=True)  \n",
        "\n",
        "model = resnet()\n",
        "#model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#model.summary()\n",
        "\n",
        "#checkpoint\n",
        "#callback_op = ModelCheckpoint(filepath='weights.{epoch:02d}.hdf5')\n",
        "callback_es = EarlyStopping(monitor='val_acc', patience=10,\n",
        "                            mode='auto', verbose=1)\n",
        "\n",
        "batch_size = 30\n",
        "num_epochs = 100 # 普通は100〜300くらいを指定することが多い\n",
        "\n",
        "#h = model.fit(X_train, Y_train,\n",
        "#               batch_size=batch_size,\n",
        "#               epochs=epochs,\n",
        "#               validation_split=0.2,\n",
        "#               callbacks=[callback_op, callback_es])\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Compute quantities required for feature-wise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(X_train, Y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  epochs=num_epochs,\n",
        "                                  validation_data=(X_test, Y_test),\n",
        "                                  steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))),\n",
        "                                  workers=1)\n",
        "\n",
        "model.save('model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1667/1667 [==============================] - 257s 154ms/step - loss: 1.6481 - acc: 0.4219 - val_loss: 1.4197 - val_acc: 0.5307\n",
            "Epoch 2/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 1.1563 - acc: 0.5942 - val_loss: 1.0373 - val_acc: 0.6553\n",
            "Epoch 3/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.9009 - acc: 0.6872 - val_loss: 0.7827 - val_acc: 0.7380\n",
            "Epoch 4/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.7595 - acc: 0.7369 - val_loss: 0.8198 - val_acc: 0.7346\n",
            "Epoch 5/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.6588 - acc: 0.7734 - val_loss: 0.5847 - val_acc: 0.8029\n",
            "Epoch 6/100\n",
            "1667/1667 [==============================] - 249s 150ms/step - loss: 0.5944 - acc: 0.7939 - val_loss: 0.5331 - val_acc: 0.8170\n",
            "Epoch 7/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.5401 - acc: 0.8163 - val_loss: 0.7370 - val_acc: 0.7746\n",
            "Epoch 8/100\n",
            "1667/1667 [==============================] - 249s 150ms/step - loss: 0.4971 - acc: 0.8288 - val_loss: 0.5934 - val_acc: 0.8033\n",
            "Epoch 9/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.4592 - acc: 0.8399 - val_loss: 0.4303 - val_acc: 0.8562\n",
            "Epoch 10/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.4294 - acc: 0.8527 - val_loss: 0.4950 - val_acc: 0.8459\n",
            "Epoch 11/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.3999 - acc: 0.8614 - val_loss: 0.5002 - val_acc: 0.8396\n",
            "Epoch 12/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.3821 - acc: 0.8685 - val_loss: 0.4008 - val_acc: 0.8706\n",
            "Epoch 13/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.3620 - acc: 0.8742 - val_loss: 0.4450 - val_acc: 0.8616\n",
            "Epoch 14/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.3382 - acc: 0.8848 - val_loss: 0.4657 - val_acc: 0.8630\n",
            "Epoch 15/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.3250 - acc: 0.8881 - val_loss: 0.3816 - val_acc: 0.8733\n",
            "Epoch 16/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.3040 - acc: 0.8942 - val_loss: 0.4206 - val_acc: 0.8680\n",
            "Epoch 17/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.2940 - acc: 0.8976 - val_loss: 0.3917 - val_acc: 0.8782\n",
            "Epoch 18/100\n",
            "1667/1667 [==============================] - 249s 150ms/step - loss: 0.2790 - acc: 0.9028 - val_loss: 0.5075 - val_acc: 0.8530\n",
            "Epoch 19/100\n",
            "1667/1667 [==============================] - 249s 149ms/step - loss: 0.2708 - acc: 0.9065 - val_loss: 0.4320 - val_acc: 0.8741\n",
            "Epoch 20/100\n",
            "1667/1667 [==============================] - 249s 150ms/step - loss: 0.2587 - acc: 0.9093 - val_loss: 0.3928 - val_acc: 0.8793\n",
            "Epoch 21/100\n",
            "1667/1667 [==============================] - 249s 149ms/step - loss: 0.2466 - acc: 0.9140 - val_loss: 0.3887 - val_acc: 0.8812\n",
            "Epoch 22/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.2385 - acc: 0.9176 - val_loss: 0.3866 - val_acc: 0.8846\n",
            "Epoch 23/100\n",
            "1667/1667 [==============================] - 249s 149ms/step - loss: 0.2294 - acc: 0.9189 - val_loss: 0.4121 - val_acc: 0.8812\n",
            "Epoch 24/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.2186 - acc: 0.9233 - val_loss: 0.4129 - val_acc: 0.8812\n",
            "Epoch 25/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.2121 - acc: 0.9261 - val_loss: 0.3452 - val_acc: 0.8943\n",
            "Epoch 26/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.2035 - acc: 0.9285 - val_loss: 0.3263 - val_acc: 0.9009\n",
            "Epoch 27/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1912 - acc: 0.9334 - val_loss: 0.3557 - val_acc: 0.8922\n",
            "Epoch 28/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1894 - acc: 0.9343 - val_loss: 0.3612 - val_acc: 0.8961\n",
            "Epoch 29/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1822 - acc: 0.9367 - val_loss: 0.3390 - val_acc: 0.8988\n",
            "Epoch 30/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1740 - acc: 0.9382 - val_loss: 0.4164 - val_acc: 0.8849\n",
            "Epoch 31/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1685 - acc: 0.9410 - val_loss: 0.4352 - val_acc: 0.8848\n",
            "Epoch 32/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1666 - acc: 0.9410 - val_loss: 0.3744 - val_acc: 0.8956\n",
            "Epoch 33/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1613 - acc: 0.9429 - val_loss: 0.3537 - val_acc: 0.9000\n",
            "Epoch 34/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1537 - acc: 0.9462 - val_loss: 0.3859 - val_acc: 0.8992\n",
            "Epoch 35/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1485 - acc: 0.9466 - val_loss: 0.3722 - val_acc: 0.9003\n",
            "Epoch 36/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1446 - acc: 0.9486 - val_loss: 0.3793 - val_acc: 0.8958\n",
            "Epoch 37/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1418 - acc: 0.9506 - val_loss: 0.3604 - val_acc: 0.9007\n",
            "Epoch 38/100\n",
            "1667/1667 [==============================] - 249s 150ms/step - loss: 0.1323 - acc: 0.9536 - val_loss: 0.4119 - val_acc: 0.8971\n",
            "Epoch 39/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1287 - acc: 0.9540 - val_loss: 0.4111 - val_acc: 0.8989\n",
            "Epoch 40/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1314 - acc: 0.9530 - val_loss: 0.4064 - val_acc: 0.8954\n",
            "Epoch 41/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1237 - acc: 0.9558 - val_loss: 0.4216 - val_acc: 0.8952\n",
            "Epoch 42/100\n",
            "1667/1667 [==============================] - 250s 150ms/step - loss: 0.1171 - acc: 0.9585 - val_loss: 0.4056 - val_acc: 0.8992\n",
            "Epoch 43/100\n",
            "1667/1667 [==============================] - 249s 150ms/step - loss: 0.1195 - acc: 0.9580 - val_loss: 0.3712 - val_acc: 0.9035\n",
            "Epoch 44/100\n",
            "1667/1667 [==============================] - 249s 150ms/step - loss: 0.1166 - acc: 0.9593 - val_loss: 0.4041 - val_acc: 0.9007\n",
            "Epoch 45/100\n",
            "1534/1667 [==========================>...] - ETA: 19s - loss: 0.1112 - acc: 0.9605"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aXMHK5aDGvhZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data Augumentation のパラメータを変更するも制度は向上せず\n",
        "\n",
        "        rotation_range=45,  \n",
        "        width_shift_range=0.4, \n",
        "        height_shift_range=0.4,  "
      ]
    },
    {
      "metadata": {
        "id": "Vm0KWP_VphFx",
        "colab_type": "code",
        "outputId": "614db172-524d-41a3-9576-3af2584ed1ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1546
        }
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# ResNet のモデルを定義するセル\n",
        "#\n",
        "\n",
        "# inputs と residual 2つの処理結果を受け取って、両者を足す\n",
        "\n",
        "# inputs：ショートカットせずに畳み込みやReLuなどをしたあとの信号\n",
        "# residual：畳み込みなどを通らずショートカットして流れてくる信号のこと、つまりモジュールに入ってくる信号と同じ\n",
        "def _shortcut(inputs, residual):\n",
        "  \n",
        "  # residual のほうのフィルタ数を取得する\n",
        "  # ちなみにデフォルトでは\n",
        "  # _keras_shape[1] 画像の幅\n",
        "  # _keras_shape[2] 画像の高さ\n",
        "  # _keras_shape[3] チャンネル数\n",
        "  # チャンネル数、幅、高さの順番のこともあるが、今回はデフォルトでOK\n",
        "  n_filters = residual._keras_shape[3]\n",
        "  \n",
        "  # inputs と residual とでチャネル数が違うかもしれない。\n",
        "  # そのままだと足せないので、1x1 conv を使って residual 側のフィルタ数に合わせている\n",
        "  shortcut = Convolution2D(n_filters, (1,1), strides=(1,1),kernel_initializer='glorot_normal', padding='same')(inputs)\n",
        " \n",
        "  # 2つを足す\n",
        "  return add([shortcut, residual])\n",
        "\n",
        "\n",
        "# ResBlock を定義\n",
        "# ここでの処理は BatchNorm → ReLU → Conv とシンプルなものにしてあるが、\n",
        "# ここを色々変更する改良案が無数にある\n",
        "###ResBlock とは、ショートカット構造のあるレイヤーの組み合わせ（ブロック）のこと\n",
        "def _resblock(n_filters, strides=(1,1)):\n",
        "  def f(inputs):  \n",
        "    weight_decay = 0.01\n",
        "#    x = Convolution2D(n_filters, (1,1), strides=strides,kernel_initializer='he_normal', padding='same', kernel_regularizer=regularizers.l2(weight_decay), activity_regularizer=regularizers.l1(weight_decay))(inputs)\n",
        "    x = Convolution2D(n_filters, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "#    x = Convolution2D(n_filters, (3,3), strides=strides,kernel_initializer='he_normal', padding='same', kernel_regularizer=regularizers.l2(weight_decay), activity_regularizer=regularizers.l1(weight_decay))(x)    \n",
        "    x = Convolution2D(n_filters, (3,3), strides=strides,kernel_initializer='he_normal', padding='same')(x)    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "#    x = Convolution2D(n_filters * 4, (1,1), strides=strides,kernel_initializer='he_normal', padding='same', kernel_regularizer=regularizers.l2(weight_decay), activity_regularizer=regularizers.l1(weight_decay))(x)\n",
        "    x = Convolution2D(n_filters * 4, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    # 入力そのものと、BN→ReLU→Conv したものとを足す\n",
        "    # この部分がResNetのもっとも重要な点\n",
        "    return _shortcut(inputs, x)\n",
        "  \n",
        "  return f\n",
        "\n",
        "def resnet():\n",
        "    inputs = Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = BatchNormalization()(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=64)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=64)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "#    x = Dropout(0.2)(x)\n",
        "  \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=128)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=128)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "    \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "  \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(10, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "\n",
        "model = resnet()\n",
        "opt = Adam(amsgrad=True)  \n",
        "#model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#model.summary()\n",
        "\n",
        "#checkpoint\n",
        "#callback_op = ModelCheckpoint(filepath='weights.{epoch:02d}.hdf5')\n",
        "callback_es = EarlyStopping(monitor='val_acc', patience=10,\n",
        "                            mode='auto', verbose=1)\n",
        "\n",
        "batch_size = 30\n",
        "num_epochs = 100 # 普通は100〜300くらいを指定することが多い\n",
        "\n",
        "#h = model.fit(X_train, Y_train,\n",
        "#               batch_size=batch_size,\n",
        "#               epochs=epochs,\n",
        "#               validation_split=0.2,\n",
        "#               callbacks=[callback_op, callback_es])\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.4,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.4,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Compute quantities required for feature-wise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(X_train, Y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  epochs=num_epochs,\n",
        "                                  validation_data=(X_test, Y_test),\n",
        "                                  steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))),\n",
        "                                  workers=1)\n",
        "\n",
        "model.save('model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1667/1667 [==============================] - 261s 157ms/step - loss: 1.8426 - acc: 0.3439 - val_loss: 1.8715 - val_acc: 0.3542\n",
            "Epoch 2/100\n",
            "1667/1667 [==============================] - 253s 152ms/step - loss: 1.5227 - acc: 0.4557 - val_loss: 1.3784 - val_acc: 0.5096\n",
            "Epoch 3/100\n",
            "1667/1667 [==============================] - 253s 152ms/step - loss: 1.2881 - acc: 0.5436 - val_loss: 1.3553 - val_acc: 0.5642\n",
            "Epoch 4/100\n",
            "1667/1667 [==============================] - 267s 160ms/step - loss: 1.1222 - acc: 0.6068 - val_loss: 1.0625 - val_acc: 0.6636\n",
            "Epoch 5/100\n",
            "1667/1667 [==============================] - 255s 153ms/step - loss: 1.0121 - acc: 0.6462 - val_loss: 0.9310 - val_acc: 0.6917\n",
            "Epoch 6/100\n",
            "1667/1667 [==============================] - 253s 152ms/step - loss: 0.9238 - acc: 0.6779 - val_loss: 0.8341 - val_acc: 0.7267\n",
            "Epoch 7/100\n",
            "1667/1667 [==============================] - 254s 152ms/step - loss: 0.8598 - acc: 0.7058 - val_loss: 0.8574 - val_acc: 0.7291\n",
            "Epoch 8/100\n",
            "1667/1667 [==============================] - 253s 152ms/step - loss: 0.8083 - acc: 0.7203 - val_loss: 0.7451 - val_acc: 0.7544\n",
            "Epoch 9/100\n",
            "1667/1667 [==============================] - 255s 153ms/step - loss: 0.7617 - acc: 0.7363 - val_loss: 0.7894 - val_acc: 0.7569\n",
            "Epoch 10/100\n",
            "1667/1667 [==============================] - 256s 153ms/step - loss: 0.7220 - acc: 0.7486 - val_loss: 0.6736 - val_acc: 0.7873\n",
            "Epoch 11/100\n",
            "1667/1667 [==============================] - 258s 155ms/step - loss: 0.6965 - acc: 0.7597 - val_loss: 0.8272 - val_acc: 0.7667\n",
            "Epoch 12/100\n",
            " 597/1667 [=========>....................] - ETA: 3:07 - loss: 0.6649 - acc: 0.7710"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a4e587c72099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m                                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                                   \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                                   workers=1)\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "hiwN6oS_H55U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "weight_decay = 0.01\n",
        "kernel_regularizer=regularizers.l2(weight_decay), activity_regularizer=regularizers.l1(weight_decay)\n",
        "\n",
        "上記の正則化はあまりこうかなし\n",
        "\n",
        "↓_resblockの後の Batch Normalization と Activation の削除も精度が悪化したので、\n",
        "data augumentation のパラメータを調整し再実行\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "lrmUgSSO2fCL",
        "colab_type": "code",
        "outputId": "42d1bc60-5e85-44fb-f957-8a6967b368be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2234
        }
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# ResNet のモデルを定義するセル\n",
        "#\n",
        "\n",
        "# inputs と residual 2つの処理結果を受け取って、両者を足す\n",
        "\n",
        "# inputs：ショートカットせずに畳み込みやReLuなどをしたあとの信号\n",
        "# residual：畳み込みなどを通らずショートカットして流れてくる信号のこと、つまりモジュールに入ってくる信号と同じ\n",
        "def _shortcut(inputs, residual):\n",
        "  \n",
        "  # residual のほうのフィルタ数を取得する\n",
        "  # ちなみにデフォルトでは\n",
        "  # _keras_shape[1] 画像の幅\n",
        "  # _keras_shape[2] 画像の高さ\n",
        "  # _keras_shape[3] チャンネル数\n",
        "  # チャンネル数、幅、高さの順番のこともあるが、今回はデフォルトでOK\n",
        "  n_filters = residual._keras_shape[3]\n",
        "  \n",
        "  # inputs と residual とでチャネル数が違うかもしれない。\n",
        "  # そのままだと足せないので、1x1 conv を使って residual 側のフィルタ数に合わせている\n",
        "  shortcut = Convolution2D(n_filters, (1,1), strides=(1,1),kernel_initializer='glorot_normal', padding='same')(inputs)\n",
        " \n",
        "  # 2つを足す\n",
        "  return add([shortcut, residual])\n",
        "\n",
        "\n",
        "# ResBlock を定義\n",
        "# ここでの処理は BatchNorm → ReLU → Conv とシンプルなものにしてあるが、\n",
        "# ここを色々変更する改良案が無数にある\n",
        "###ResBlock とは、ショートカット構造のあるレイヤーの組み合わせ（ブロック）のこと\n",
        "def _resblock(n_filters, strides=(1,1)):\n",
        "  def f(inputs):  \n",
        "    x = Convolution2D(n_filters, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters, (3,3), strides=strides,kernel_initializer='he_normal', padding='same')(x)    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(n_filters * 4, (1,1), strides=strides,kernel_initializer='he_normal', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    # 入力そのものと、BN→ReLU→Conv したものとを足す\n",
        "    # この部分がResNetのもっとも重要な点\n",
        "    return _shortcut(inputs, x)\n",
        "  \n",
        "  return f\n",
        "\n",
        "def resnet():\n",
        "    inputs = Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = BatchNormalization()(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=64)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=64)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "#    x = Dropout(0.2)(x)\n",
        "  \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=128)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=128)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "    \n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = MaxPooling2D(strides=(2,2))(x)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = _resblock(n_filters=512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "#    x = _resblock(n_filters=256)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = Activation('relu')(x)\n",
        "    #    x = Dropout(0.2)(x)\n",
        "  \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(10, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "  \n",
        "opt = Adam(amsgrad=True)  \n",
        "\n",
        "model = resnet()\n",
        "#model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#model.summary()\n",
        "\n",
        "#checkpoint\n",
        "#callback_op = ModelCheckpoint(filepath='weights.{epoch:02d}.hdf5')\n",
        "callback_es = EarlyStopping(monitor='val_acc', patience=10,\n",
        "                            mode='auto', verbose=1)\n",
        "\n",
        "batch_size = 30\n",
        "num_epochs = 100 # 普通は100〜300くらいを指定することが多い\n",
        "\n",
        "#h = model.fit(X_train, Y_train,\n",
        "#               batch_size=batch_size,\n",
        "#               epochs=epochs,\n",
        "#               validation_split=0.2,\n",
        "#               callbacks=[callback_op, callback_es])\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=40,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        fill_mode='nearest',\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Compute quantities required for feature-wise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(X_train, Y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  epochs=num_epochs,\n",
        "                                  validation_data=(X_test, Y_test),\n",
        "                                  steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))),\n",
        "                                  workers=1)\n",
        "\n",
        "model.save('model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1667/1667 [==============================] - 317s 190ms/step - loss: 1.7322 - acc: 0.3796 - val_loss: 1.4686 - val_acc: 0.4938\n",
            "Epoch 2/100\n",
            "1667/1667 [==============================] - 306s 184ms/step - loss: 1.3385 - acc: 0.5235 - val_loss: 1.4442 - val_acc: 0.5270\n",
            "Epoch 3/100\n",
            "1667/1667 [==============================] - 307s 184ms/step - loss: 1.0915 - acc: 0.6191 - val_loss: 0.9541 - val_acc: 0.6823\n",
            "Epoch 4/100\n",
            "1667/1667 [==============================] - 306s 183ms/step - loss: 0.9347 - acc: 0.6749 - val_loss: 0.8924 - val_acc: 0.7069\n",
            "Epoch 5/100\n",
            "1667/1667 [==============================] - 305s 183ms/step - loss: 0.8354 - acc: 0.7102 - val_loss: 0.7936 - val_acc: 0.7412\n",
            "Epoch 6/100\n",
            "1667/1667 [==============================] - 307s 184ms/step - loss: 0.7644 - acc: 0.7353 - val_loss: 0.8018 - val_acc: 0.7489\n",
            "Epoch 7/100\n",
            "1667/1667 [==============================] - 307s 184ms/step - loss: 0.6996 - acc: 0.7595 - val_loss: 0.6804 - val_acc: 0.7816\n",
            "Epoch 8/100\n",
            "1667/1667 [==============================] - 308s 185ms/step - loss: 0.6498 - acc: 0.7767 - val_loss: 0.6108 - val_acc: 0.8026\n",
            "Epoch 9/100\n",
            "1667/1667 [==============================] - 309s 186ms/step - loss: 0.6144 - acc: 0.7868 - val_loss: 0.6561 - val_acc: 0.7905\n",
            "Epoch 10/100\n",
            "1667/1667 [==============================] - 308s 185ms/step - loss: 0.5727 - acc: 0.8034 - val_loss: 0.6104 - val_acc: 0.8070\n",
            "Epoch 11/100\n",
            "1667/1667 [==============================] - 307s 184ms/step - loss: 0.5403 - acc: 0.8142 - val_loss: 0.5564 - val_acc: 0.8245\n",
            "Epoch 12/100\n",
            "1667/1667 [==============================] - 307s 184ms/step - loss: 0.5128 - acc: 0.8225 - val_loss: 0.5406 - val_acc: 0.8302\n",
            "Epoch 13/100\n",
            "1667/1667 [==============================] - 305s 183ms/step - loss: 0.4905 - acc: 0.8307 - val_loss: 0.4943 - val_acc: 0.8460\n",
            "Epoch 14/100\n",
            "1667/1667 [==============================] - 307s 184ms/step - loss: 0.4697 - acc: 0.8372 - val_loss: 0.5509 - val_acc: 0.8286\n",
            "Epoch 15/100\n",
            "1667/1667 [==============================] - 307s 184ms/step - loss: 0.4550 - acc: 0.8437 - val_loss: 0.4885 - val_acc: 0.8443\n",
            "Epoch 16/100\n",
            "1667/1667 [==============================] - 307s 184ms/step - loss: 0.4312 - acc: 0.8497 - val_loss: 0.4869 - val_acc: 0.8450\n",
            "Epoch 17/100\n",
            "1667/1667 [==============================] - 306s 183ms/step - loss: 0.4197 - acc: 0.8542 - val_loss: 0.5218 - val_acc: 0.8413\n",
            "Epoch 18/100\n",
            "1667/1667 [==============================] - 304s 183ms/step - loss: 0.4027 - acc: 0.8597 - val_loss: 0.4672 - val_acc: 0.8556\n",
            "Epoch 19/100\n",
            "1667/1667 [==============================] - 304s 183ms/step - loss: 0.3902 - acc: 0.8651 - val_loss: 0.4341 - val_acc: 0.8640\n",
            "Epoch 20/100\n",
            "1667/1667 [==============================] - 307s 184ms/step - loss: 0.3761 - acc: 0.8689 - val_loss: 0.3953 - val_acc: 0.8747\n",
            "Epoch 21/100\n",
            "1667/1667 [==============================] - 304s 182ms/step - loss: 0.3627 - acc: 0.8758 - val_loss: 0.4802 - val_acc: 0.8587\n",
            "Epoch 22/100\n",
            "1667/1667 [==============================] - 305s 183ms/step - loss: 0.3506 - acc: 0.8791 - val_loss: 0.4429 - val_acc: 0.8654\n",
            "Epoch 23/100\n",
            "1667/1667 [==============================] - 305s 183ms/step - loss: 0.3364 - acc: 0.8828 - val_loss: 0.4143 - val_acc: 0.8712\n",
            "Epoch 24/100\n",
            "1667/1667 [==============================] - 304s 182ms/step - loss: 0.3257 - acc: 0.8870 - val_loss: 0.4043 - val_acc: 0.8751\n",
            "Epoch 25/100\n",
            "1667/1667 [==============================] - 304s 182ms/step - loss: 0.3200 - acc: 0.8878 - val_loss: 0.3901 - val_acc: 0.8795\n",
            "Epoch 26/100\n",
            "1667/1667 [==============================] - 305s 183ms/step - loss: 0.3086 - acc: 0.8934 - val_loss: 0.4243 - val_acc: 0.8732\n",
            "Epoch 27/100\n",
            "1667/1667 [==============================] - 303s 182ms/step - loss: 0.3009 - acc: 0.8959 - val_loss: 0.4514 - val_acc: 0.8674\n",
            "Epoch 28/100\n",
            "1667/1667 [==============================] - 304s 182ms/step - loss: 0.2931 - acc: 0.8970 - val_loss: 0.3691 - val_acc: 0.8914\n",
            "Epoch 29/100\n",
            "1667/1667 [==============================] - 294s 176ms/step - loss: 0.2863 - acc: 0.9006 - val_loss: 0.3739 - val_acc: 0.8870\n",
            "Epoch 30/100\n",
            "1667/1667 [==============================] - 256s 153ms/step - loss: 0.2791 - acc: 0.9030 - val_loss: 0.3874 - val_acc: 0.8890\n",
            "Epoch 31/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.2709 - acc: 0.9048 - val_loss: 0.4003 - val_acc: 0.8838\n",
            "Epoch 32/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.2611 - acc: 0.9088 - val_loss: 0.3907 - val_acc: 0.8846\n",
            "Epoch 33/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.2576 - acc: 0.9115 - val_loss: 0.3894 - val_acc: 0.8853\n",
            "Epoch 34/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.2514 - acc: 0.9126 - val_loss: 0.4179 - val_acc: 0.8761\n",
            "Epoch 35/100\n",
            "1667/1667 [==============================] - 256s 153ms/step - loss: 0.2444 - acc: 0.9146 - val_loss: 0.3655 - val_acc: 0.8937\n",
            "Epoch 36/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.2361 - acc: 0.9175 - val_loss: 0.4261 - val_acc: 0.8817\n",
            "Epoch 37/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.2294 - acc: 0.9192 - val_loss: 0.4111 - val_acc: 0.8812\n",
            "Epoch 38/100\n",
            "1667/1667 [==============================] - 256s 153ms/step - loss: 0.2298 - acc: 0.9201 - val_loss: 0.3559 - val_acc: 0.8941\n",
            "Epoch 39/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.2264 - acc: 0.9205 - val_loss: 0.3331 - val_acc: 0.9021\n",
            "Epoch 40/100\n",
            "1667/1667 [==============================] - 256s 153ms/step - loss: 0.2150 - acc: 0.9238 - val_loss: 0.4615 - val_acc: 0.8772\n",
            "Epoch 41/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.2162 - acc: 0.9241 - val_loss: 0.4010 - val_acc: 0.8857\n",
            "Epoch 42/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.2082 - acc: 0.9268 - val_loss: 0.3396 - val_acc: 0.9046\n",
            "Epoch 43/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.2028 - acc: 0.9291 - val_loss: 0.4347 - val_acc: 0.8837\n",
            "Epoch 44/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.1991 - acc: 0.9309 - val_loss: 0.3440 - val_acc: 0.9035\n",
            "Epoch 45/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.1992 - acc: 0.9304 - val_loss: 0.3743 - val_acc: 0.8978\n",
            "Epoch 46/100\n",
            "1667/1667 [==============================] - 256s 153ms/step - loss: 0.1877 - acc: 0.9329 - val_loss: 0.4435 - val_acc: 0.8855\n",
            "Epoch 47/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.1871 - acc: 0.9340 - val_loss: 0.3916 - val_acc: 0.8944\n",
            "Epoch 48/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.1839 - acc: 0.9357 - val_loss: 0.4496 - val_acc: 0.8815\n",
            "Epoch 49/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.1815 - acc: 0.9370 - val_loss: 0.4080 - val_acc: 0.8923\n",
            "Epoch 50/100\n",
            "1667/1667 [==============================] - 256s 153ms/step - loss: 0.1779 - acc: 0.9378 - val_loss: 0.3671 - val_acc: 0.8960\n",
            "Epoch 51/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.1714 - acc: 0.9397 - val_loss: 0.3481 - val_acc: 0.9029\n",
            "Epoch 52/100\n",
            "1667/1667 [==============================] - 256s 153ms/step - loss: 0.1681 - acc: 0.9395 - val_loss: 0.4021 - val_acc: 0.8974\n",
            "Epoch 53/100\n",
            "1667/1667 [==============================] - 256s 153ms/step - loss: 0.1701 - acc: 0.9397 - val_loss: 0.3715 - val_acc: 0.8976\n",
            "Epoch 54/100\n",
            "1667/1667 [==============================] - 256s 153ms/step - loss: 0.1644 - acc: 0.9423 - val_loss: 0.3872 - val_acc: 0.8984\n",
            "Epoch 55/100\n",
            "1667/1667 [==============================] - 256s 153ms/step - loss: 0.1603 - acc: 0.9431 - val_loss: 0.4343 - val_acc: 0.8933\n",
            "Epoch 56/100\n",
            "1667/1667 [==============================] - 256s 153ms/step - loss: 0.1567 - acc: 0.9447 - val_loss: 0.3573 - val_acc: 0.9016\n",
            "Epoch 57/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.1541 - acc: 0.9459 - val_loss: 0.4259 - val_acc: 0.8921\n",
            "Epoch 58/100\n",
            "1667/1667 [==============================] - 256s 154ms/step - loss: 0.1524 - acc: 0.9458 - val_loss: 0.4322 - val_acc: 0.8916\n",
            "Epoch 59/100\n",
            "1667/1667 [==============================] - 257s 154ms/step - loss: 0.1524 - acc: 0.9467 - val_loss: 0.3835 - val_acc: 0.8984\n",
            "Epoch 60/100\n",
            "1667/1667 [==============================] - 257s 154ms/step - loss: 0.1484 - acc: 0.9471 - val_loss: 0.4080 - val_acc: 0.8947\n",
            "Epoch 61/100\n",
            "1667/1667 [==============================] - 257s 154ms/step - loss: 0.1449 - acc: 0.9496 - val_loss: 0.4123 - val_acc: 0.8957\n",
            "Epoch 62/100\n",
            "1667/1667 [==============================] - 257s 154ms/step - loss: 0.1415 - acc: 0.9504 - val_loss: 0.3834 - val_acc: 0.9028\n",
            "Epoch 63/100\n",
            " 120/1667 [=>............................] - ETA: 3:47 - loss: 0.1370 - acc: 0.9503"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}